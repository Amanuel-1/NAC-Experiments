{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/bek/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import string as s\n",
    "from nltk.corpus import stopwords\n",
    "# import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# import seaborn as sns\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only the specified number of rows\n",
    "train = pd.read_csv('../../data/ag-news/train.csv', nrows=25000)\n",
    "test = pd.read_csv('../../data/ag-news/test.csv', nrows=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Index                                              Title  \\\n",
       "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2            3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3            3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4            3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         Description  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Fears for T N pension after talks</td>\n",
       "      <td>Unions representing workers at Turner   Newall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
       "      <td>SPACE.com - TORONTO, Canada -- A second\\team o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP)</td>\n",
       "      <td>AP - A company founded by a chemistry research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP)</td>\n",
       "      <td>AP - It's barely dawn when Mike Fitzpatrick st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Calif. Aims to Limit Farm-Related Smog (AP)</td>\n",
       "      <td>AP - Southern California's smog-fighting agenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Index                                              Title  \\\n",
       "0            3                  Fears for T N pension after talks   \n",
       "1            4  The Race is On: Second Private Team Sets Launc...   \n",
       "2            4      Ky. Company Wins Grant to Study Peptides (AP)   \n",
       "3            4      Prediction Unit Helps Forecast Wildfires (AP)   \n",
       "4            4        Calif. Aims to Limit Farm-Related Smog (AP)   \n",
       "\n",
       "                                         Description  \n",
       "0  Unions representing workers at Turner   Newall...  \n",
       "1  SPACE.com - TORONTO, Canada -- A second\\team o...  \n",
       "2  AP - A company founded by a chemistry research...  \n",
       "3  AP - It's barely dawn when Mike Fitzpatrick st...  \n",
       "4  AP - Southern California's smog-fighting agenc...  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "(5000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train.Description\n",
    "test_x = test.Description\n",
    "\n",
    "train_y =  train['Class Index']\n",
    "test_y = test['Class Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.apply(striphtml)\n",
    "test_x = test_x.apply(striphtml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(data):\n",
    "    return  re.sub(r'\\s*(?:https?://)?www\\.\\S*\\.[A-Za-z]{2,5}\\s*', ' ', data).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.apply(remove_url)\n",
    "test_x = test_x.apply(remove_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "def word_tok(data):\n",
    "    tokens = re.findall(\"[\\w']+\", data)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.apply(word_tok)\n",
    "test_x =test_x.apply(word_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "    stopWords = stopwords.words('english')\n",
    "    new_list = []\n",
    "    for i in data:\n",
    "        if i.lower() not in stopWords:\n",
    "            new_list.append(i)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.apply(remove_stopwords)\n",
    "test_x = test_x.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of Punctuation Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(data):\n",
    "    new_list = []\n",
    "    for i in data:\n",
    "        for j in s.punctuation:\n",
    "            i = i.replace(j,'')\n",
    "        new_list.append(i)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.apply(remove_punctuations)\n",
    "test_x = test_x.apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_number(data):\n",
    "    no_digit_list = []\n",
    "    new_list = []\n",
    "\n",
    "    for i in data:\n",
    "        for j in s.digits:\n",
    "            i = i.replace(j,'')\n",
    "        no_digit_list.append(i)\n",
    "\n",
    "    for i in no_digit_list:\n",
    "        if i!='':\n",
    "            new_list.append(i)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.apply(remove_number)\n",
    "test_x = test_x.apply(remove_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(data):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    roots = [lemmatizer.lemmatize(i) for i in data]\n",
    "    return roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.apply(lemmatization)\n",
    "test_x = test_x.apply(lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove extra words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extraWords(data):\n",
    "    extra_words =['href','iii','lt','gt','ii','com','quot']\n",
    "\n",
    "    new_list = []\n",
    "    for i in data:\n",
    "        if i not in extra_words:\n",
    "            new_list.append(i)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.apply(remove_extraWords)\n",
    "test_x = test_x.apply(remove_extraWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [\" \".join(map(str, lst)) for lst in train_x]\n",
    "test_x = [\" \".join(map(str, lst)) for lst in test_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of features extracted\n",
      "13742\n",
      "['aa' 'aapl' 'aaron' 'aaron peirsol' 'ab' 'abandon' 'abandoned' 'abbey'\n",
      " 'abbey national' 'abby' 'abby wambach' 'abc' 'abducted' 'abdul'\n",
      " 'abdullah' 'ability' 'able' 'aboard' 'abortion' 'abroad' 'abrupt'\n",
      " 'abruptly' 'absence' 'abu' 'abu ghraib' 'abu ghraib prison' 'abu hamza'\n",
      " 'abuja' 'abuse' 'abuse scandal' 'abusing' 'abusing iraqi' 'ac' 'academic'\n",
      " 'academy' 'accelerate' 'accelerated' 'accelerating' 'accept' 'accepted'\n",
      " 'access' 'access brokerage' 'accessing' 'accident' 'acclaim'\n",
      " 'acclaim entertainment' 'accord' 'according' 'according company'\n",
      " 'according data' 'according latest' 'according new'\n",
      " 'according new report' 'according new study' 'according report'\n",
      " 'according report released' 'according research' 'according statement'\n",
      " 'according study' 'according study released' 'according survey' 'account'\n",
      " 'accounting' 'accounting firm' 'accounting practice' 'accounting scandal'\n",
      " 'accurate' 'accusation' 'accuse' 'accused' 'accused plotting' 'accuser'\n",
      " 'accuses' 'accusing' 'accusing software' 'accusing software giant' 'ace'\n",
      " 'achieve' 'achieved' 'achievement' 'acknowledged' 'acquire' 'acquired'\n",
      " 'acquiring' 'acquisition' 'acquitted' 'across' 'across board'\n",
      " 'across country' 'across nation' 'act' 'acting' 'action' 'action lawsuit'\n",
      " 'activated' 'active' 'activist' 'activity' 'actor' 'actually']\n"
     ]
    }
   ],
   "source": [
    "# Feature are extracted from the datset and TF-IDF (term frequency - Inverse Documents Frequence ) is used for this purpose\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "TfIdf = TfidfVectorizer(min_df=8,ngram_range=(1,3))\n",
    "train_1 = TfIdf.fit_transform(train_x)\n",
    "test_1 = TfIdf.transform(test_x)\n",
    "\n",
    "print(\"no of features extracted\")\n",
    "print(len(TfIdf.get_feature_names_out()))\n",
    "print(TfIdf.get_feature_names_out()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_1.toarray()\n",
    "test_x = test_1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 13742)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 13742)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Convert Series to NumPy array before reshaping\n",
    "train_y = encoder.fit_transform(train_y.to_numpy().reshape(-1, 1))\n",
    "test_y = encoder.transform(test_y.to_numpy().reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 4)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training data\n",
    "np.save('train_x.npy', train_x)\n",
    "np.save('train_y.npy', train_y)\n",
    "\n",
    "# Save test data\n",
    "np.save('test_x.npy', test_x)\n",
    "np.save('test_y.npy', test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
